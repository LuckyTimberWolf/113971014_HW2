# -*- coding: utf-8 -*-
"""traditional_methods.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jjyhFVgocbN212RakeAMtkXs6y5pn-Pm
"""

import math
import jieba
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
import string

# 繁體中文標點符號 (用於分句)
CHINESE_PUNCTUATION = '。！？；…'
# 停用詞 (從作業提供的列表擴充，或使用 chinese-stop-words 套件)
STOP_WORDS = set(['的', '了', '在', '是', '我', '有', '和', '就', '不', '也', '很', '人', '到', '都', '一個', '上', '說', '要', '去', '你', '們'])

def segment_text(text):
    """使用 jieba 進行精確模式分詞，並移除停用詞"""
    # 建議先載入自定義詞典
    # jieba.load_userdict('custom_dict.txt')
    words = jieba.cut(text, cut_all=False)
    return [word.lower() for word in words if word.strip() and word.lower() not in STOP_WORDS and word not in string.punctuation and word not in CHINESE_PUNCTUATION]

# --- 測試資料 ---
documents = [
    "人工智慧正在改變世界,機器學習是其核心技術",
    "深度學習推動了人工智慧的發展,特別是在圖像識別領域",
    "今天天氣很好,適合出去運動",
    "機器學習和深度學習都是人工智慧的重要分支",
    "運動有益健康,每天都應該保持運動習慣"
]

"""### --- A-1: TF-IDF 文本相似度計算 (20分) ---"""

def calculate_tf(word_dict, total_words):
    """
    計算詞頻 (TF)
    Args:
        word_dict: 詞彙計數字典
        total_words: 總詞數
    Returns:
        tf_dict: TF 值字典
    """
    tf_dict = {}
    # 提示: TF(t, d) = (詞 t 在文件 d 的出現次數) / (文件 d 的總詞數)
    for word, count in word_dict.items():
        # TF = 詞彙計數 / 總詞數
        tf_dict[word] = count / total_words
    return tf_dict

def calculate_idf(documents, word):
    """
    計算逆文件 fréquence (IDF)
    Args:
        documents: 文件列表 (分好詞的)
        word: 目標詞彙
    Returns:
        idf: IDF 值
    """
    # 提示: IDF(t, D) = log( (文件總數) / (1 + 包含詞 t 的文件數) )
    # (加 1 是為了避免分母為 0)
    N = len(documents) # 總文件數
    df = 0 # 包含該詞彙的文件數 (Document Frequency, DF)
    for document in documents:
        # 檢查該文件（已分詞或原始文本）中是否包含該詞彙
        # 這裡假設 documents 已經是原始文本列表 [cite: 54]
        if word in document:
            df += 1

    # IDF 公式: log(N / (DF + 1))，+1 是為了避免除以零
    # 這裡使用自然對數 (math.log)
    idf = math.log(N / (df + 1)) if df > 0 else 0
    return idf # [cite: 47]

def calculate_tfidf(documents):
    # 1. 使用 jieba 對所有文件進行分詞
    segmented_documents = [segment_text(doc) for doc in documents]

    # 2. 收集所有獨特的詞彙
    all_words = set()
    for doc_words in segmented_documents:
        all_words.update(doc_words)

    # 3. 計算所有詞彙的 IDF 值
    idf_values = {word: calculate_idf(documents, word) for word in all_words}

    # 4. 遍歷所有文件,計算每個詞的 TF 和 TF-IDF
    tfidf_vectors = []
    for doc_words in segmented_documents:
        word_counts = Counter(doc_words) # 計算詞頻
        total_words_in_doc = len(doc_words)

        tfidf_vector = {} # 當前文件的 TF-IDF 向量
        if total_words_in_doc > 0: # 避免除以零
            tf_values = calculate_tf(word_counts, total_words_in_doc)
            for word, tf_val in tf_values.items():
                # TF-IDF = TF * IDF
                tfidf_vector[word] = tf_val * idf_values.get(word, 0) # 使用 .get() 避免 KeyError，如果詞不在 all_words 中則 IDF 為 0
        tfidf_vectors.append(tfidf_vector)

    return tfidf_vectors

tfidf_vectors = calculate_tfidf(documents)
print("TF-IDF Vectors:")
for i, vector in enumerate(tfidf_vectors):
    print(f"Document {i+1}: {vector}")

# 2. 使用 scikit-learn 實作(5分)
def sklearn_tfidf_similarity(documents):
    """
    使用 sklearn 計算 TF-IDF 並返回餘弦相似度矩陣
    """
    # 提示:
    # 1. TfidfVectorizer().fit_transform(documents)
    # 2. cosine_similarity()
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(documents)
    cosine_sim = cosine_similarity(tfidf_matrix)
    return cosine_sim, vectorizer.get_feature_names_out()

cosine_sim_matrix, feature_names = sklearn_tfidf_similarity(documents)
print("Cosine Similarity Matrix:")
print(cosine_sim_matrix)
print("\nFeature Names:")
print(feature_names)

"""### --- A-2: 基於規則的文本分類 (15分) ---

#####情感分類器
"""

test_texts = [
  "這家餐廳的牛肉麵真的太好吃了,湯頭濃郁,麵條Q彈,下次一定再來!",
  "最新的AI技術突破讓人驚艷,深度學習模型的表現越來越好",
  "這部電影劇情空洞,演技糟糕,完全是浪費時間",
  "每天慢跑5公里,配合適當的重訓,體能進步很多"
]

class RuleBasedSentimentClassifier:
    def __init__(self):
        # 建立正負面詞彙庫
        self.positive_words = set(['好','棒','優秀','喜歡','推薦', '滿意','開心','值得','精彩','完美'])
        self.negative_words = set(['差','糟','失望','討厭','不推薦', '浪費','無聊','爛','糟糕','差勁'])
        # 加入否定詞處理
        self.negation_words = set(['不','沒','無','非','別'])
        # 初始化程度副詞字典
        self.degree_words = {}

    def classify(self, text):
        """
        分類邏輯:
        1. 計算正負詞數量
        2. 處理否定詞 (否定詞 + 正面詞 = 負面)
        3. (選做) 考慮程度副詞的加權
        4. 返回: "正面" / "負面" / "中性"
        """
        # 您的實作
        # 提示:
        # 1. 使用 jieba.cut 分詞
        # 2. 遍歷詞彙, 檢查是否在 negation_words, positive_words, negative_words
        # 3. 處理 "不" + "好" 的情況
        tokens = segment_text(text)
        sentiment_score = 0

        for i, word in enumerate(tokens):
            weight = 1.0
            # 處理程度副詞
            weight = self.degree_words.get(word, 1.0)

            if word in self.positive_words:
                # 檢查前一個詞是否為否定詞
                is_negated = (i > 0) and (tokens[i-1] in self.negation_words)
                if is_negated:
                    sentiment_score -= weight
                else:
                    sentiment_score += weight
            elif word in self.negative_words:
                # 檢查前一個詞是否為否定詞 (雙重否定算正面，簡化處理為: 否定+負面詞仍為負面)
                is_negated = (i > 0) and (tokens[i-1] in self.negation_words)
                if is_negated:
                    # 否定+負面 -> 轉正面
                    sentiment_score += weight
                else:
                    sentiment_score -= weight

        if sentiment_score > 0.5:
            return "正面"
        elif sentiment_score < -0.5:
            return "負面"
        else:
            return "中性"



classifier = RuleBasedSentimentClassifier()

print("Sentiment Analysis Results:")
for text in test_texts:
    sentiment = classifier.classify(text)
    print(f"Text: '{text}'")
    print(f"Sentiment: {sentiment}\n")

"""#####主題分類器"""

class TopicClassifier:
    def __init__(self):
        self.topic_keywords = {
            '科技': ['AI','人工智慧','電腦','軟體','程式','演算法'],
            '運動': ['運動','健身','跑步','游泳','球類','比賽'],
            '美食': ['吃','食物','餐廳','美味','料理','烹飪'],
            '旅遊': ['旅行','景點','飯店','機票','觀光','度假']
        }

    def classify(self, text):
        """
        返回最可能的主題
        """
        # 您的實作
        # 提示:
        # 1. 計算文本中每個主題的關鍵詞出現次數
        # 2. 返回次數最高的主題
        tokens = segment_text(text)
        topic_scores = {topic: 0 for topic in self.topic_keywords}

        for token in tokens:
            for topic, keywords in self.topic_keywords.items():
                if token in keywords:
                    topic_scores[topic] += 1

        # 找到分數最高的主題
        max_score = max(topic_scores.values())
        if max_score > 0:
            # 返回分數最高的主題
            return max(topic_scores, key=topic_scores.get)
        else:
            return "其他"

"""### --- A-3: 統計式自動摘要 (15分) ---"""

article = """
人工智慧(AI)的發展正在深刻改變我們的生活方式。從早上起床時的智慧鬧鐘,
到通勤時的路線規劃,再到工作中的各種輔助工具,AI無處不在。

在醫療領域,AI協助醫生進行疾病診斷,提高了診斷的准確率和效率。透過分析
大量的醫療影像和病歷資料,AI能夠發現人眼容易忽略的細節,為患者提供更好
的治療方案。

教育方面,AI個人化學習系統能夠根據每個學生的學習進度和特點,提供客製化
的教學內容。這種因材施教的方式,讓學習變得更加高效和有趣。

然而,AI的快速發展也帶來了一些挑戰。首先是就業問題,許多傳統工作可能會
被AI取代。其次是隱私和安全問題,AI系統需要大量數據來訓練,如何保護個人
隱私成為重要議題。最後是倫理問題,AI的决策過程往往缺乏透明度,可能會產
生偏見或歧視。

面對這些挑戰,我們需要在推動AI發展的同時,建立相應的法律法規和倫理準則。
只有這樣,才能確保AI技術真正為人類福祉服務,創造一個更美好的未來。
"""

class StatisticalSummarizer:
    def __init__(self):
        # 載入停用詞
        self.stop_words = set(["的", "了", "在", "是", "我", "有", "和", "就", "不", "也", "很"])

    def _split_sentences(self, text):
        """處理中文標點進行分句"""
        sentences = re.split(f'([{CHINESE_PUNCTUATION}])', text)
        result = []
        for i in range(0, len(sentences), 2):
            if i + 1 < len(sentences):
                # 將句子和標點符號重新組合
                result.append(sentences[i] + sentences[i+1])
            else:
                result.append(sentences[i])
        return [s.strip() for s in result if s.strip()]


    def sentence_score(self, sentence, word_freq, total_words, all_sentences):
        """
        計算句子重要性分數
        考慮因素:
        1. 包含高頻詞的數量 (word_freq)
        2. 句子位置 (首尾句加權)
        3. 句子長度 (太短或太長扣分)
        4. (選做) 是否包含數字或專有名詞
        """
        tokens = segment_text(sentence)
        score = 0

        # 1. 包含高頻詞的數量 (權重因子)
        for token in tokens:
            if token in word_freq:
                # 可以用詞頻的 log 值或歸一化值作為權重
                score += math.log(word_freq[token] + 1)

        # 2. 句子位置 (首尾句加權)
        sentence_index = all_sentences.index(sentence)
        N = len(all_sentences)
        if sentence_index == 0 or sentence_index == N - 1:
            score *= 1.5 # 提高首尾句權重

        # 3. 句子長度 (太短或太長扣分)
        if len(tokens) < 5 or len(tokens) > 30:
            score *= 0.8

        # 4. 是否包含數字或專有名詞 (可選，作為加分項)
        if any(re.search(r'\d', t) for t in tokens):
            score *= 1.1

        return score

    def summarize(self, text, ratio=0.3):
        """
        生成摘要步驟:
        1. 分句 (處理中文標點)
        2. 分詞並計算詞頻 (記得移除停用詞)
        3. 計算每個句子的重要性分數
        4. 選擇最高分的句子 (根據 ratio 決定數量)
        5. 按原文順序排列
        """
        # 您的實作
        # 提示:
        # 1. 分句: 可用 re.split(r'[。！？]', text)
        # 2. 分詞: jieba.cut
        # 3. 詞頻: Counter()
        # 4. 句子評分: 呼叫 self.sentence_score
        # 5. 排序: sorted(..., key=lambda x: x['score'], reverse=True)
        # 6. 按原順序重排: sorted(..., key=lambda x: x['position'])
        # 1. 分句
        sentences = self._split_sentences(text)
        # 2. 分詞並計算詞頻
        all_tokens = [token for sentence in sentences for token in segment_text(sentence)]
        word_freq = Counter(all_tokens)
        total_words = len(all_tokens)

        # 3. 計算每個句子的重要性分數
        sentence_scores = {}
        for sentence in sentences:
            sentence_scores[sentence] = self.sentence_score(sentence, word_freq, total_words, sentences)

        # 4. 選擇最高分的句子
        # 決定要選取的句子數量
        num_sentences = max(1, math.ceil(len(sentences) * ratio))

        # 根據分數排序，選出 top N 句子
        sorted_sentences = sorted(sentence_scores.items(), key=lambda item: item[1], reverse=True)
        top_sentences = [s[0] for s in sorted_sentences[:num_sentences]]

        # 5. 按原文順序排列 (保持邏輯連貫性)
        summary = ""
        for sentence in sentences:
            if sentence in top_sentences:
                summary += sentence + " "

        return summary.strip()

summarizer = StatisticalSummarizer()
summary = summarizer.summarize(article, ratio=0.4)
print("Article Summary:")
print(summary)